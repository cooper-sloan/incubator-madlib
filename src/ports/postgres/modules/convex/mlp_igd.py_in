# coding=utf-8

"""
@file mlp_igd.py_in

@brief Multilayer perceptron using IGD: Driver functions

@namespace mlp_igd
"""
import plpy

from utilities.control import MinWarning
from utilities.utilities import py_list_to_sql_string
from utilities.utilities import extract_keyvalue_params
from utilities.validate_args import output_tbl_valid
from utilities.validate_args import input_tbl_valid
from utilities.validate_args import is_var_valid
from utilities.validate_args import table_exists
from utilities.validate_args import get_expr_type
from utilities.utilities import _assert


def mlp(schema_madlib, source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_param_str, activation, is_classification, **kwargs):
    """
    Args:
        @param schema_madlib
        @param source_table
        @param output_table
        @param independent_varname
        @param dependent_varname
        @param hidden_layer_sizes
        @param optimizer_param_str

    Returns:
        None
    """
    with MinWarning('info'):


        optimizer_params = _get_optimizer_params(optimizer_param_str)
        _validate_args(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params, is_classification)

        n_iterations = 1
        prev_state = None
        tolerance = optimizer_params["tolerance"]
        activation_name = _get_activation_function_name(activation)
        activation_index = _get_activation_index(activation_name)
        if(is_classification):
            dependent_variable_sql = """
            SELECT DISTINCT {dependent_varname} FROM {source_table};
            """.format(dependent_varname=dependent_varname,
                       source_table=source_table)
            labels = plpy.execute(dependent_variable_sql)
            one_hot_dependent_varname='ARRAY['
            for x in labels:
                label = _format_label(x[dependent_varname])
                one_hot_dependent_varname+=dependent_varname+"="+str(label)+","
            # Remove the last comma
            one_hot_dependent_varname = one_hot_dependent_varname[:-1]
            one_hot_dependent_varname+=']::integer[]'
            dependent_varname=one_hot_dependent_varname
        while True:
            if prev_state:
                prev_state_str = py_list_to_sql_string(prev_state, array_type="double precision")
            else:
                prev_state_str = "(NULL)::DOUBLE PRECISION[]"
            train_sql = """
                SELECT
                    {schema_madlib}.mlp_igd_step(
                        ({independent_varname})::DOUBLE PRECISION[],
                        ({dependent_varname})::DOUBLE PRECISION[],
                        {prev_state},
                        {hidden_layer_sizes},
                        ({step_size})::FLOAT8,
                        {activation},
                        {is_classification}) as curr_state
                FROM {source_table} AS _src""".format(
                schema_madlib=schema_madlib,
                independent_varname=independent_varname,
                dependent_varname=dependent_varname,
                prev_state=prev_state_str,
                hidden_layer_sizes=py_list_to_sql_string(hidden_layer_sizes, array_type="integer"),
                step_size=optimizer_params["step_size"],
                source_table=source_table,
                activation=activation_index,
                is_classification=int(is_classification))
            curr_state = plpy.execute(train_sql)[0]["curr_state"]
            dist_sql = """
                SELECT {schema_madlib}.internal_mlp_igd_distance(
                        {prev_state},
                        {curr_state}) as state_dist
                """.format(schema_madlib=schema_madlib,
                           prev_state=prev_state_str,
                           curr_state=py_list_to_sql_string(curr_state, "double precision"),
                           tolerance=tolerance)
            state_dist = plpy.execute(dist_sql)[0]["state_dist"]
            if ((state_dist and state_dist < tolerance) or
                    n_iterations > optimizer_params["n_iterations"]):
                break
            prev_state = curr_state
            n_iterations += 1
        _build_output_tables(schema_madlib, output_table, curr_state, n_iterations)
# ----------------------------------------------------------------------


def _build_output_tables(schema_madlib, output_table, final_state, n_iterations):
    final_state_str = py_list_to_sql_string(final_state, array_type="double precision")

    model_table_query = """
        CREATE TABLE {output_table} AS
            SELECT
                (result).coeff AS coeff,
                (result).loss           AS loss,
                {n_iterations}                       AS num_iterations
                -- (result).num_rows_processed     AS num_rows_processed,
                -- n_tuples_including_nulls - (result).num_rows_processed
            FROM
            (
                SELECT
                    {schema_madlib}.internal_mlp_igd_result(
                        {final_state_str}
                    ) AS result
            ) rel_state_subq
        """.format(**locals())
    plpy.execute(model_table_query)
# ----------------------------------------------------------------------


def _get_optimizer_params(param_str):
    params_defaults = {
        "step_size": (0.01, float),
        "n_iterations": (100, int),
        "n_tries": (1, int),
        "tolerance": (0.001, float),
    }
    param_defaults = dict([(k, v[0]) for k, v in params_defaults.items()])
    param_types = dict([(k, v[1]) for k, v in params_defaults.items()])

    if not param_str:
        return param_defaults, param_str

    name_value = extract_keyvalue_params(param_str, param_types, param_defaults,
                                         ignore_invalid=True)
    return name_value
# ----------------------------------------------------------------------

def _validate_args_classification(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params):
    pass
# ----------------------------------------------------------------------

def _validate_args_regression(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params):
    _assert("[]" in get_expr_type(dependent_varname,source_table),"Dependent variable column should refer to an array")
# ----------------------------------------------------------------------


def _validate_args(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params, is_classification):
    """
    Args:
        @param kwargs

    Returns:

    """
    input_tbl_valid(source_table, "MLP")
    output_tbl_valid(output_table, "MLP")
    _assert(is_var_valid(source_table, independent_varname),
            "MLP error: invalid independent_varname "
            "('{independent_varname}') for source_table "
            "({source_table})!".format(independent_varname=independent_varname,
                                       source_table=source_table))

    _assert(is_var_valid(source_table, dependent_varname),
            "MLP error: invalid dependent_varname "
            "('{dependent_varname}') for source_table "
            "({source_table})!".format(dependent_varname=dependent_varname,
                                       source_table=source_table))


    _assert(all(type(value) is int for value in hidden_layer_sizes), "MLP error: Hidden layers sizes must be integers")
    _assert(all(value >= 0 for value in hidden_layer_sizes),"MLP error: Hidden layers sizes must be greater than 0.")
    _assert(optimizer_params["tolerance"]>=0,"MLP error: Tolerance should be greater than or equal to 0.")
    _assert(optimizer_params["n_tries"]>=1,"MLP error: Number of tries should be greater than or equal to 1")
    _assert(optimizer_params["n_iterations"]>=1,"MLP error: Number of iterations should be greater than or equal to 1")
    _assert(optimizer_params["step_size"]>0,"MLP error: Stepsize should be greater than 0.")
    _assert("[]" in get_expr_type(independent_varname,source_table),"Independent variable column should refer to an array")
    if is_classification:
        _validate_args_classification(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params)
    else:
        _validate_args_regression(source_table, output_table, independent_varname,
        dependent_varname, hidden_layer_sizes,
        optimizer_params)
# ----------------------------------------------------------------------




def _get_activation_function_name(activation_function):
    if not activation_function:
        activation_function = 'sigmoid'
    else:
        # Add non-linear kernels below after implementing them.
        supported_activation_function = ['sigmoid', 'tanh', 'relu']
        try:
            # allow user to specify a prefix substring of
            # supported kernels. This works because the supported
            # kernels have unique prefixes.
            activation_function = next(x for x in supported_activation_function
                    if x.startswith(activation_function))
        except StopIteration:
            # next() returns a StopIteration if no element found
            plpy.error("SVM Error: Invalid activation function: "
                       "{0}. Supported activation functions are ({1})"
                       .format(activation_function, ','.join(sorted(supported_activation_function))))
    return activation_function
# ------------------------------------------------------------------------------

def _get_activation_index(activation_name):
    table = {"relu":0,"sigmoid":1,"tanh":2}
    return table[activation_name]

def _format_label(label):
    if type(label) is str:
        return "'" + label + "'"
    return label
